I plan to use this page to record the papers I have read.

## Classical

 - **Imagenet classification with deep convolutional neural networks**. A Krizhevsky et.al. **Commun. ACM**, **2012**, **Number of Citations: **127575, ([pdf](.\Papers\Imagenet_classification_with_deep_convolutional_neural_networks.pdf))([link](https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html)).
 - **Deep residual learning for image recognition**. K He et.al. **CoDIT**, **2016**, **Number of Citations: **211374, ([pdf](.\Papers\Deep_residual_learning_for_image_recognition.pdf))([link](http://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html)).
 - **Bleu: a method for automatic evaluation of machine translation**. K Papineni et.al. **ACL**, **2002**, **Number of Citations: **26918, ([pdf](.\Papers\Bleu_a_method_for_automatic_evaluation_of_machine_translation.pdf))([link](https://aclanthology.org/P02-1040.pdf)).
 - **Attention is all you need**. A Vaswani et.al. **IEEE Signal Process. Lett.**, **2017**, **Number of Citations: **115085, ([pdf](.\Papers\Attention_is_all_you_need.pdf))([link](https://proceedings.neurips.cc/paper/7181-attention-is-all)).
 - {{A gentle introduction to graph neural networks}}
 - **Bert: Pre-training of deep bidirectional transformers for language understanding**. J Devlin et.al. **arXiv preprint arXiv â€¦**, **2018**, **Number of Citations: **96635, ([pdf](.\Papers\Bert_Pre-training_of_deep_bidirectional_transformers_for_language_understanding.pdf))([link](https://arxiv.org/abs/1810.04805)).


## RAG